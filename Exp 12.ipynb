{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ef6a50",
   "metadata": {},
   "source": [
    "# Method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91c444f",
   "metadata": {},
   "source": [
    "# Create a new Python file, and import the following packages: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c4719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6608c6b5",
   "metadata": {},
   "source": [
    "#  Define a function to extract features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0d2c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(word_list):\n",
    " return dict([(word, True) for word in word_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1498c25e",
   "metadata": {},
   "source": [
    "# We need training data for this, so we will use movie reviews in NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af192575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d91d511",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "   # Load positive and negative reviews  \n",
    "   positive_fileids = movie_reviews.fileids('pos')\n",
    "   negative_fileids = movie_reviews.fileids('neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f302f94",
   "metadata": {},
   "source": [
    "# Let’s separate these into positive and negative reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d1d3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_positive = [(extract_features(movie_reviews.words(fileids=[f])), 'Positive') for f in positive_fileids]\n",
    "features_negative = [(extract_features(movie_reviews.words(fileids=[f])),'Negative') for f in negative_fileids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa3e5d0",
   "metadata": {},
   "source": [
    "#  Divide the data into training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e1e7f88",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-17-ec30f8123df2>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-17-ec30f8123df2>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    threshold_factor = 0.8\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test (80/20)\n",
    " threshold_factor = 0.8\n",
    "  threshold_positive = int(threshold_factor * len(features_positive))\n",
    "  threshold_negative = int(threshold_factor * len(features_negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eac22d",
   "metadata": {},
   "source": [
    "# Extract the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0515133f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-22-1354fd7cd3dc>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-22-1354fd7cd3dc>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    features_test = features_positive[threshold_positive:] + features_negative[threshold_negative:]\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "features_train = features_positive[:threshold_positive] + features_negative[:threshold_negative]\n",
    "   features_test = features_positive[threshold_positive:] + features_negative[threshold_negative:]  \n",
    "   print \"\\nNumber of training datapoints:\", len(features_train)\n",
    "   print \"Number of test datapoints:\", len(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb6b080",
   "metadata": {},
   "source": [
    "# We will use a Naive Bayes classifier. Define the object and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes classifier\n",
    "   classifier = NaiveBayesClassifier.train(features_train)\n",
    "   print \"\\nAccuracy of the classifier:\", nltk.classify.util.accuracy(classifier, features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ded954",
   "metadata": {},
   "source": [
    "# The classifier object contains the most informative words that it obtained during analysis. These words basically have a strong say in what’s classified as a positive or a negative review. Let’s print them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c1fb99e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(\"\\nTop 10 most informative words:\")? (<ipython-input-23-f25dca7b544f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-f25dca7b544f>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    print \"\\nTop 10 most informative words:\"\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(\"\\nTop 10 most informative words:\")?\n"
     ]
    }
   ],
   "source": [
    "print \"\\nTop 10 most informative words:\"\n",
    "   for item in classifier.most_informative_features()[:10]:\n",
    "       print item[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef274351",
   "metadata": {},
   "source": [
    "# Create a couple of random input sentences: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9052d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input reviews\n",
    "   input_reviews = [\n",
    "       \"It is an amazing movie\", \n",
    "       \"This is a dull movie. I would never recommend it to anyone.\",\n",
    "       \"The cinematography is pretty great in this movie\", \n",
    "       \"The direction was terrible and the story was all over the place\" \n",
    "   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b20c9f4",
   "metadata": {},
   "source": [
    "# Run the classifier on those input sentences and obtain the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2f0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"\\nPredictions:\"\n",
    "   for review in input_reviews:\n",
    "       print \"\\nReview:\", review\n",
    "       probdist = classifier.prob_classify(extract_features(review.split()))\n",
    "       pred_sentiment = probdist.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc42229c",
   "metadata": {},
   "source": [
    "# Print the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e11f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Predicted sentiment:\", pred_sentiment \n",
    "       print \"Probability:\", round(probdist.prob(pred_sentiment), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be18f104",
   "metadata": {},
   "source": [
    "# Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be368fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2073d465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import nltk\n",
    "\n",
    "nltk.download([\"names\",\"stopwords\",\"state_union\",\"twitter_samples\",\"movie_reviews\",\"averaged_perceptron_tagger\",\"vader_lexicon\",\"punkt\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2243f4e2",
   "metadata": {},
   "source": [
    "# Using NLTK’s Pre-Trained Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77b670f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc018ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a113b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in words if w.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c188374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For', 'some', 'quick', 'analysis', ',', 'creating', 'a', 'corpus', 'could',\n",
      " 'be', 'overkill', '.', 'If', 'all', 'you', 'need', 'is', 'a', 'word', 'list',\n",
      " ',', 'there', 'are', 'simpler', 'ways', 'to', 'achieve', 'that', 'goal', '.']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "text = \"\"\"\n",
    "For some quick analysis, creating a corpus could be overkill.\n",
    "If all you need is a word list,\n",
    "there are simpler ways to achieve that goal.\"\"\"\n",
    "pprint(nltk.word_tokenize(text), width=79, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dc83d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.295, 'pos': 0.705, 'compound': 0.8012}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"Wow, NLTK is really powerful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d787ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [t.replace(\"://\", \"//\") for t in nltk.corpus.twitter_samples.strings()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fafbd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> True RT @genophilia: The greatest con job of the 20th century. #WhiteGenocide #CulturalMarxism #ukip #pegida #scotnight #nrx #tcot http//t.co/t…\n",
      "> False #VoteSNP  https//t.co/lNDutHby3O\n",
      "> False @UKIP nigel marry me\n",
      "> True @Saskia_TeamLH44 Thanks baby girl. You too!! :) xx\n",
      "> False Performance being the operative word.... acting ..... https//t.co/7eFt5Kr7KZ\n",
      "> False Never seeing your dad until midnight bc he worked hard as fuck :( #GrowingUpPoor\n",
      "> False Srsly, Y U do that? :((  https//t.co/g0r01GGj2b\n",
      "> False RT @PigeonJon: ◻️Tory.\n",
      "\n",
      "◻️ Labour.\n",
      "\n",
      "◻️ Lib Dem.\n",
      "\n",
      "◻️ UKIP.\n",
      "\n",
      "☑️ Pigeon.\n",
      "> True @ChrisBourne see you later :)\n",
      "> True RT @afneil: Guardian/ICM poll: 44% of viewers  that Cameron was \"best on the night\", compared to 38% for Miliband and 19% for Clegg\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def is_positive(tweet: str) -> bool:\n",
    "    \"\"\"True if tweet has positive compound sentiment, False otherwise.\"\"\"\n",
    "    return sia.polarity_scores(tweet)[\"compound\"] > 0\n",
    "\n",
    "shuffle(tweets)\n",
    "for tweet in tweets[:10]:\n",
    "    print(\">\", is_positive(tweet), tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebb7b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "negative_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "all_review_ids = positive_review_ids + negative_review_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0d841c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def is_positive(review_id: str) -> bool:\n",
    "    \"\"\"True if the average of all sentence compound scores is positive.\"\"\"\n",
    "    text = nltk.corpus.movie_reviews.raw(review_id)\n",
    "    scores = [\n",
    "        sia.polarity_scores(sentence)[\"compound\"]\n",
    "        for sentence in nltk.sent_tokenize(text)\n",
    "    ]\n",
    "    return mean(scores) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29e9eed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.00% correct\n"
     ]
    }
   ],
   "source": [
    "shuffle(all_review_ids)\n",
    "correct = 0\n",
    "for review_id in all_review_ids:\n",
    "    if is_positive(review_id):\n",
    "        if review_id in positive_review_ids:\n",
    "            correct += 1\n",
    "    else:\n",
    "        if review_id in negative_review_ids:\n",
    "            correct += 1\n",
    "\n",
    "print(F\"{correct / len(all_review_ids):.2%} correct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a804f6b3",
   "metadata": {},
   "source": [
    "# Customizing NLTK’s Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3ffb128",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "def skip_unwanted(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if not word.isalpha() or word in unwanted:\n",
    "        return False\n",
    "    if tag.startswith(\"NN\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "positive_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"pos\"]))\n",
    ")]\n",
    "negative_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"neg\"]))\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba168868",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_fd = nltk.FreqDist(positive_words)\n",
    "negative_fd = nltk.FreqDist(negative_words)\n",
    "\n",
    "common_set = set(positive_fd).intersection(negative_fd)\n",
    "\n",
    "for word in common_set:\n",
    "    del positive_fd[word]\n",
    "    del negative_fd[word]\n",
    "\n",
    "top_100_positive = {word for word, count in positive_fd.most_common(100)}\n",
    "top_100_negative = {word for word, count in negative_fd.most_common(100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3188cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "positive_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"pos\"])\n",
    "    if w.isalpha() and w not in unwanted\n",
    "])\n",
    "negative_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"neg\"])\n",
    "    if w.isalpha() and w not in unwanted\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a6fd8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    features = dict()\n",
    "    wordcount = 0\n",
    "    compound_scores = list()\n",
    "    positive_scores = list()\n",
    "\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word.lower() in top_100_positive:\n",
    "                wordcount += 1\n",
    "        compound_scores.append(sia.polarity_scores(sentence)[\"compound\"])\n",
    "        positive_scores.append(sia.polarity_scores(sentence)[\"pos\"])\n",
    "\n",
    "    # Adding 1 to the final compound score to always have positive numbers\n",
    "    # since some classifiers you'll use later don't work with negative numbers.\n",
    "    features[\"mean_compound\"] = mean(compound_scores) + 1\n",
    "    features[\"mean_positive\"] = mean(positive_scores)\n",
    "    features[\"wordcount\"] = wordcount\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdd7a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"pos\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "]\n",
    "features.extend([\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"neg\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b513d8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               wordcount = 3                 pos : neg    =     10.4 : 1.0\n",
      "               wordcount = 2                 pos : neg    =      6.3 : 1.0\n",
      "               wordcount = 4                 pos : neg    =      6.1 : 1.0\n",
      "               wordcount = 5                 pos : neg    =      4.6 : 1.0\n",
      "               wordcount = 0                 neg : pos    =      1.8 : 1.0\n",
      "               wordcount = 1                 pos : neg    =      1.6 : 1.0\n",
      "           mean_positive = 0.09305263157894737    pos : neg    =      1.0 : 1.0\n",
      "           mean_positive = 0.10004761904761905    pos : neg    =      1.0 : 1.0\n",
      "           mean_positive = 0.162             pos : neg    =      1.0 : 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6586666666666666"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 1/4 of the set for training\n",
    "train_count = len(features) // 4\n",
    "shuffle(features)\n",
    "classifier = nltk.NaiveBayesClassifier.train(features[:train_count])\n",
    "classifier.show_most_informative_features(10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nltk.classify.accuracy(classifier, features[train_count:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5621c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
